#!/usr/bin/env python3
"""
é‡æ„éªŒè¯æµ‹è¯•ç³»ç»Ÿ
ç”¨äºéªŒè¯ç›‘å¬æ¨¡å—é‡æ„å‰åçš„åŠŸèƒ½ä¸€è‡´æ€§å’Œæ€§èƒ½è¡¨ç°
"""

import asyncio
import json
import time
import sys
import os
from pathlib import Path
from typing import Dict, List, Any, Tuple
from unittest.mock import Mock, AsyncMock
from dataclasses import dataclass, asdict
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../'))

# å¯¼å…¥æµ‹è¯•ç›¸å…³æ¨¡å—
sys.path.append(os.path.dirname(__file__))
from comprehensive_e2e_test import ComprehensiveE2ETestRunner, TestResult

@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æ•°æ®ç±»"""
    test_name: str
    execution_time: float
    memory_usage: float
    api_calls: int
    messages_processed: int
    timestamp: str

@dataclass
class RefactorValidationResult:
    """é‡æ„éªŒè¯ç»“æœ"""
    test_name: str
    before_result: TestResult
    after_result: TestResult
    performance_change: Dict[str, float]
    behavior_consistent: bool
    issues: List[str]

class RefactorValidator:
    """é‡æ„éªŒè¯å™¨"""
    
    def __init__(self):
        self.baseline_results = {}
        self.baseline_performance = {}
        self.validation_results = []
        
    def save_baseline(self, results: List[TestResult], performance: List[PerformanceBenchmark]):
        """ä¿å­˜é‡æ„å‰çš„åŸºçº¿æ•°æ®"""
        baseline_file = Path("refactor_baseline.json")
        
        baseline_data = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_results': [asdict(result) for result in results],
            'performance_benchmarks': [asdict(perf) for perf in performance],
            'total_tests': len(results),
            'success_count': sum(1 for r in results if r.success),
            'total_execution_time': sum(r.execution_time for r in results),
            'total_api_calls': sum(r.api_calls for r in results)
        }
        
        with open(baseline_file, 'w', encoding='utf-8') as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… åŸºçº¿æ•°æ®å·²ä¿å­˜åˆ° {baseline_file}")
        print(f"   æµ‹è¯•æ€»æ•°: {len(results)}")
        print(f"   æˆåŠŸæµ‹è¯•: {sum(1 for r in results if r.success)}")
        print(f"   æ€»æ‰§è¡Œæ—¶é—´: {sum(r.execution_time for r in results):.2f}ç§’")
    
    def load_baseline(self) -> Tuple[List[TestResult], List[PerformanceBenchmark]]:
        """åŠ è½½åŸºçº¿æ•°æ®"""
        baseline_file = Path("refactor_baseline.json")
        
        if not baseline_file.exists():
            raise FileNotFoundError("åŸºçº¿æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œå»ºç«‹åŸºçº¿")
        
        with open(baseline_file, 'r', encoding='utf-8') as f:
            baseline_data = json.load(f)
        
        test_results = [TestResult(**data) for data in baseline_data['test_results']]
        performance_benchmarks = [PerformanceBenchmark(**data) for data in baseline_data['performance_benchmarks']]
        
        return test_results, performance_benchmarks
    
    def compare_results(self, baseline_results: List[TestResult], 
                       current_results: List[TestResult]) -> List[RefactorValidationResult]:
        """æ¯”è¾ƒé‡æ„å‰åçš„æµ‹è¯•ç»“æœ"""
        validation_results = []
        
        # åˆ›å»ºåŸºçº¿ç»“æœçš„å­—å…¸ï¼Œä¾¿äºæŸ¥æ‰¾
        baseline_dict = {result.test_name: result for result in baseline_results}
        
        for current_result in current_results:
            test_name = current_result.test_name
            baseline_result = baseline_dict.get(test_name)
            
            if not baseline_result:
                # æ–°å¢çš„æµ‹è¯•
                validation_results.append(RefactorValidationResult(
                    test_name=test_name,
                    before_result=TestResult(test_name="N/A", success=True, details="æ–°æµ‹è¯•", execution_time=0.0),
                    after_result=current_result,
                    performance_change={},
                    behavior_consistent=True,
                    issues=["æ–°å¢æµ‹è¯•"] if current_result.success else ["æ–°æµ‹è¯•å¤±è´¥"]
                ))
                continue
            
            # æ¯”è¾ƒè¡Œä¸ºä¸€è‡´æ€§
            behavior_consistent = (
                baseline_result.success == current_result.success and
                self._compare_test_behavior(baseline_result, current_result)
            )
            
            # è®¡ç®—æ€§èƒ½å˜åŒ–
            performance_change = {
                'execution_time_change': (current_result.execution_time - baseline_result.execution_time),
                'execution_time_ratio': (current_result.execution_time / baseline_result.execution_time) if baseline_result.execution_time > 0 else 1.0,
                'api_calls_change': current_result.api_calls - baseline_result.api_calls
            }
            
            # è¯†åˆ«é—®é¢˜
            issues = []
            if not behavior_consistent:
                issues.append("è¡Œä¸ºä¸ä¸€è‡´")
            if current_result.success != baseline_result.success:
                issues.append("æˆåŠŸçŠ¶æ€æ”¹å˜")
            if performance_change['execution_time_ratio'] > 1.5:
                issues.append("æ‰§è¡Œæ—¶é—´æ˜¾è‘—å¢åŠ ")
            if performance_change['api_calls_change'] != 0:
                issues.append("APIè°ƒç”¨æ¬¡æ•°æ”¹å˜")
            
            validation_results.append(RefactorValidationResult(
                test_name=test_name,
                before_result=baseline_result,
                after_result=current_result,
                performance_change=performance_change,
                behavior_consistent=behavior_consistent,
                issues=issues
            ))
        
        return validation_results
    
    def _compare_test_behavior(self, baseline: TestResult, current: TestResult) -> bool:
        """æ¯”è¾ƒæµ‹è¯•è¡Œä¸ºçš„æ ¸å¿ƒé€»è¾‘"""
        # æ¯”è¾ƒå…³é”®è¡Œä¸ºæŒ‡æ ‡
        if baseline.success != current.success:
            return False
        
        # å¯¹äºæˆåŠŸçš„æµ‹è¯•ï¼Œæ¯”è¾ƒè¯¦ç»†ä¿¡æ¯çš„ç›¸ä¼¼æ€§
        if baseline.success and current.success:
            # å¯ä»¥æ·»åŠ æ›´ç»†ç²’åº¦çš„æ¯”è¾ƒé€»è¾‘
            # ä¾‹å¦‚ï¼Œæ¯”è¾ƒAPIè°ƒç”¨æ¬¡æ•°ã€å¤„ç†çš„æ¶ˆæ¯æ•°é‡ç­‰
            return True
        
        # å¯¹äºå¤±è´¥çš„æµ‹è¯•ï¼Œç¡®ä¿å¤±è´¥åŸå› ç±»ä¼¼
        if not baseline.success and not current.success:
            return True
        
        return True
    
    def generate_validation_report(self, validation_results: List[RefactorValidationResult]) -> Dict[str, Any]:
        """ç”Ÿæˆé‡æ„éªŒè¯æŠ¥å‘Š"""
        total_tests = len(validation_results)
        consistent_tests = sum(1 for r in validation_results if r.behavior_consistent)
        tests_with_issues = [r for r in validation_results if r.issues]
        
        # æ€§èƒ½ç»Ÿè®¡
        performance_improvements = []
        performance_regressions = []
        
        for result in validation_results:
            if 'execution_time_ratio' in result.performance_change:
                ratio = result.performance_change['execution_time_ratio']
                if ratio < 0.9:  # æ€§èƒ½æå‡è¶…è¿‡10%
                    performance_improvements.append((result.test_name, ratio))
                elif ratio > 1.1:  # æ€§èƒ½é€€åŒ–è¶…è¿‡10%
                    performance_regressions.append((result.test_name, ratio))
        
        report = {
            'summary': {
                'total_tests': total_tests,
                'consistent_behavior': consistent_tests,
                'behavior_consistency_rate': (consistent_tests / total_tests * 100) if total_tests > 0 else 0,
                'tests_with_issues': len(tests_with_issues),
                'performance_improvements': len(performance_improvements),
                'performance_regressions': len(performance_regressions)
            },
            'detailed_results': validation_results,
            'performance_analysis': {
                'improvements': performance_improvements,
                'regressions': performance_regressions
            },
            'issues_summary': self._summarize_issues(tests_with_issues),
            'recommendation': self._generate_recommendation(validation_results)
        }
        
        return report
    
    def _summarize_issues(self, tests_with_issues: List[RefactorValidationResult]) -> Dict[str, int]:
        """æ€»ç»“é—®é¢˜ç±»å‹"""
        issue_counts = {}
        for test in tests_with_issues:
            for issue in test.issues:
                issue_counts[issue] = issue_counts.get(issue, 0) + 1
        return issue_counts
    
    def _generate_recommendation(self, validation_results: List[RefactorValidationResult]) -> str:
        """ç”Ÿæˆé‡æ„å»ºè®®"""
        total_tests = len(validation_results)
        consistent_tests = sum(1 for r in validation_results if r.behavior_consistent)
        consistency_rate = (consistent_tests / total_tests * 100) if total_tests > 0 else 0
        
        if consistency_rate >= 95:
            return "âœ… é‡æ„æˆåŠŸï¼è¡Œä¸ºä¸€è‡´æ€§è¾¾åˆ°95%ä»¥ä¸Šï¼Œå¯ä»¥å®‰å…¨éƒ¨ç½²ã€‚"
        elif consistency_rate >= 85:
            return "âš ï¸ é‡æ„åŸºæœ¬æˆåŠŸï¼Œä½†å­˜åœ¨ä¸€äº›ä¸ä¸€è‡´æ€§ï¼Œå»ºè®®æ£€æŸ¥å…·ä½“é—®é¢˜åå†éƒ¨ç½²ã€‚"
        else:
            return "âŒ é‡æ„å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œè¡Œä¸ºä¸€è‡´æ€§ä½äº85%ï¼Œå¼ºçƒˆå»ºè®®ä¿®å¤åå†éªŒè¯ã€‚"
    
    def print_validation_report(self, report: Dict[str, Any]):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š é‡æ„éªŒè¯æŠ¥å‘Š")
        print("=" * 80)
        
        summary = report['summary']
        print(f"\nğŸ¯ æ€»ä½“ç»“æœ:")
        print(f"   æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"   è¡Œä¸ºä¸€è‡´: {summary['consistent_behavior']} ({summary['behavior_consistency_rate']:.1f}%)")
        print(f"   æœ‰é—®é¢˜çš„æµ‹è¯•: {summary['tests_with_issues']}")
        
        print(f"\nâš¡ æ€§èƒ½åˆ†æ:")
        print(f"   æ€§èƒ½æå‡: {summary['performance_improvements']} ä¸ªæµ‹è¯•")
        print(f"   æ€§èƒ½é€€åŒ–: {summary['performance_regressions']} ä¸ªæµ‹è¯•")
        
        # æ˜¾ç¤ºæ€§èƒ½å˜åŒ–è¯¦æƒ…
        if report['performance_analysis']['improvements']:
            print(f"\nğŸš€ æ€§èƒ½æå‡çš„æµ‹è¯•:")
            for test_name, ratio in report['performance_analysis']['improvements'][:5]:
                improvement = (1 - ratio) * 100
                print(f"   âœ… {test_name}: æå‡ {improvement:.1f}%")
        
        if report['performance_analysis']['regressions']:
            print(f"\nâš ï¸ æ€§èƒ½é€€åŒ–çš„æµ‹è¯•:")
            for test_name, ratio in report['performance_analysis']['regressions'][:5]:
                regression = (ratio - 1) * 100
                print(f"   âŒ {test_name}: é€€åŒ– {regression:.1f}%")
        
        # æ˜¾ç¤ºé—®é¢˜æ€»ç»“
        if report['issues_summary']:
            print(f"\nğŸ” é—®é¢˜ç±»å‹ç»Ÿè®¡:")
            for issue_type, count in report['issues_summary'].items():
                print(f"   {issue_type}: {count} ä¸ªæµ‹è¯•")
        
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"   {report['recommendation']}")
        
        print(f"\nâœ… éªŒè¯æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")

async def establish_baseline():
    """å»ºç«‹é‡æ„å‰çš„åŸºçº¿"""
    print("ğŸ—ï¸ å»ºç«‹é‡æ„åŸºçº¿...")
    
    # è¿è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•
    runner = ComprehensiveE2ETestRunner()
    report = await runner.run_all_tests()
    
    # åˆ›å»ºæ€§èƒ½åŸºå‡†
    performance_benchmarks = []
    for result in report['test_results']:
        benchmark = PerformanceBenchmark(
            test_name=result.test_name,
            execution_time=result.execution_time,
            memory_usage=0.0,  # å¯ä»¥æ·»åŠ å†…å­˜ä½¿ç”¨ç›‘æ§
            api_calls=result.api_calls,
            messages_processed=1,  # å¯ä»¥ä»æµ‹è¯•ç»“æœä¸­æå–
            timestamp=time.strftime('%Y-%m-%d %H:%M:%S')
        )
        performance_benchmarks.append(benchmark)
    
    # ä¿å­˜åŸºçº¿
    validator = RefactorValidator()
    validator.save_baseline(report['test_results'], performance_benchmarks)
    
    return report

async def validate_refactor():
    """éªŒè¯é‡æ„ç»“æœ"""
    print("ğŸ” éªŒè¯é‡æ„ç»“æœ...")
    
    validator = RefactorValidator()
    
    try:
        # åŠ è½½åŸºçº¿æ•°æ®
        baseline_results, baseline_performance = validator.load_baseline()
        print(f"âœ… åŠ è½½åŸºçº¿æ•°æ®: {len(baseline_results)} ä¸ªæµ‹è¯•ç»“æœ")
        
        # è¿è¡Œå½“å‰æµ‹è¯•
        runner = ComprehensiveE2ETestRunner()
        current_report = await runner.run_all_tests()
        
        # æ¯”è¾ƒç»“æœ
        validation_results = validator.compare_results(baseline_results, current_report['test_results'])
        
        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = validator.generate_validation_report(validation_results)
        validator.print_validation_report(report)
        
        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        report_file = Path(f"refactor_validation_report_{int(time.time())}.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            # è½¬æ¢ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            serializable_report = {
                'summary': report['summary'],
                'performance_analysis': report['performance_analysis'],
                'issues_summary': report['issues_summary'],
                'recommendation': report['recommendation'],
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            json.dump(serializable_report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯¦ç»†éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file}")
        
        return report
        
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        print("è¯·å…ˆè¿è¡Œ 'python refactor_validation.py --baseline' å»ºç«‹åŸºçº¿")
        return None

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç›‘å¬æ¨¡å—é‡æ„éªŒè¯å·¥å…·')
    parser.add_argument('--baseline', action='store_true', help='å»ºç«‹é‡æ„å‰çš„åŸºçº¿')
    parser.add_argument('--validate', action='store_true', help='éªŒè¯é‡æ„ç»“æœ')
    
    args = parser.parse_args()
    
    if args.baseline:
        await establish_baseline()
    elif args.validate:
        await validate_refactor()
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  å»ºç«‹åŸºçº¿: python refactor_validation.py --baseline")
        print("  éªŒè¯é‡æ„: python refactor_validation.py --validate")

if __name__ == "__main__":
    asyncio.run(main()) 